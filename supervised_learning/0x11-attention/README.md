# 0x11. Attention

## Resources:books:
Read or watch:
* [Attention Model Intuition](https://intranet.hbtn.io/rltoken/7Ot5OlxrjDuAPyQdtHYxqA)
* [Attention Model](https://intranet.hbtn.io/rltoken/pw8xV6DMI1yMKY05Rzhq0g)
* [How Transformers work in deep learning and NLP: an intuitive introduction](https://intranet.hbtn.io/rltoken/iBgxQFTCee-R1IVsYUcQFw)
* [Transformers](https://intranet.hbtn.io/rltoken/LcGkrx9evf-rTX-ELG4WtQ)
* [Bert, GPT : The Illustrated GPT-2 - Visualizing Transformer Language Models](https://intranet.hbtn.io/rltoken/BuJV0Fw7-lcXW06xxVhE5g)
* [SQuAD](https://intranet.hbtn.io/rltoken/hj1iAmCUDCnH72SGnTxsXA)
* [Glue](https://intranet.hbtn.io/rltoken/T17sCQuLN3yT8MlZ955AGA)
* [Self supervised learning](https://intranet.hbtn.io/rltoken/6wjtunlD93Ajbz1Ss1l_lQ)

---
## Learning Objectives:bulb:
What you should learn from this project:

* What is the attention mechanism?
* How to apply attention to RNNs
* What is a transformer?
* How to create an encoder-decoder transformer model
* What is GPT? 
* What is BERT?
* What is self-supervised learning?
* How to use BERT for specific NLP tasks
* What is SQuAD? GLUE?

---
---

## Author
* **Gabriel Cifuentes** - [gcifuentess](https://github.com/gcifuentess)