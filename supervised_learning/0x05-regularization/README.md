# 0x05. Regularization

## Resources:books:
Read or watch:
* [Regularization (mathematics)](https://intranet.hbtn.io/rltoken/G22TZHYwwb0PwlAuEZdDEQ)
* [An Overview of Regularization Techniques in Deep Learning](https://intranet.hbtn.io/rltoken/Mao_NUBBiwm0Qh8b-axAgw)
* [L2 Regularization and Back-Propagation](https://intranet.hbtn.io/rltoken/AY80ruaSMDL_AGnjZOpWGQ)
* [Intuitions on L1 and L2 Regularisation](https://intranet.hbtn.io/rltoken/6vsM__Tfvz7-XaNB4DHB3A)
* [Analysis of Dropout](https://intranet.hbtn.io/rltoken/huRNIkxWr5OV1Tit658LcQ)
* [Early stopping](https://intranet.hbtn.io/rltoken/4YMCmw41ovvYtMvr-Wl7LA)
* [How to use early stopping properly for training deep neural network?](https://intranet.hbtn.io/rltoken/t6UPkGJXD_nK7TfGwE9Rig)
* [Data Augmentation | How to use Deep Learning when you have Limited Dataâ€Š](https://intranet.hbtn.io/rltoken/MaLMSTSCPux71mW1RIhiBA)
* [deeplearning.ai](https://intranet.hbtn.io/rltoken/GriJE79Gr4BF8HG2DGpbYg)
* [Regularization](https://intranet.hbtn.io/rltoken/BJoxOnJN-GJyZ_fJ9qT0EQ)
* [Why Regularization Reduces Overfitting](https://intranet.hbtn.io/rltoken/dLdv5Gi77DmWNyR3MHe69g)
* [Dropout Regularization](https://intranet.hbtn.io/rltoken/23ue4EQxNd9LOCW0Q6FNNQ)
* [Understanding Dropout](https://intranet.hbtn.io/rltoken/eleB8ZvoJiOltULeHkDvGQ)
* [Other Regularization Methods](https://intranet.hbtn.io/rltoken/QuFgq0_MKTGq6UAKj5OjEw)

---
## Learning Objectives:bulb:
What you should learn from this project:

* What is regularization? What is its purpose?
* What is are L1 and L2 regularization? What is the difference between the two methods?
* What is dropout?
* What is early stopping?
* What is data augmentation?
* How do you implement the above regularization methods in Numpy? Tensorflow?
* What are the pros and cons of the above regularization methods?

---
---

## Author
* **Gabriel Cifuentes** - [gcifuentess](https://github.com/gcifuentess)