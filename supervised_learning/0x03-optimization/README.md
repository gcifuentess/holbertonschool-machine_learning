# 0x03. Optimization

## Resources:books:
Read or watch:
* [Hyperparameter (machine learning)](https://intranet.hbtn.io/rltoken/1QBGvpFW16wWkdzpbPQ3DQ)
* [Feature scaling](https://intranet.hbtn.io/rltoken/w-mu-1FTMnCw_bo51x1H1w)
* [Why, How and When to Scale your Features](https://intranet.hbtn.io/rltoken/GGzAGiwPp84A1_Oz9KL3tQ)
* [Normalizing your data](https://intranet.hbtn.io/rltoken/qAJARRbV2HbG-xSTK_Ioxw)
* [Moving average](https://intranet.hbtn.io/rltoken/_Na-3oh6JT9YqhWnxE5cRg)
* [An overview of gradient descent optimization algorithms](https://intranet.hbtn.io/rltoken/-TMwJwWHSavWMohuQ5yQvA)
* [A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size](https://intranet.hbtn.io/rltoken/-Bpr2w5FmPvMnmn-EHW6Rw)
* [Stochastic Gradient Descent with momentum](https://intranet.hbtn.io/rltoken/M0KoVyrrTqNNIl-jwB9oaw)
* [Understanding RMSprop](https://intranet.hbtn.io/rltoken/YLEmEHIbXfOVsmtvH6tpSA)
* [Adam](https://intranet.hbtn.io/rltoken/mvig4z79fIisqwNZEfK0rg)
* [Learning Rate Schedules](https://intranet.hbtn.io/rltoken/PXUlwt9b8kKIatyqpvloIQ)
* [deeplearning.ai](https://intranet.hbtn.io/rltoken/-tmvgOpWb_sjJzR5hv6VyA)
* [Normalizing Inputs](https://intranet.hbtn.io/rltoken/cXcbUMLDZZHhvQb9jDQweg)
* [Mini Batch Gradient Descent](https://intranet.hbtn.io/rltoken/BI4l4WlyRrmNLbjpfJPLCQ)
* [Understanding Mini-Batch Gradient Descent](https://intranet.hbtn.io/rltoken/dsdZXmqN6wm9EC4HuQOD6g)
* [Exponentially Weighted Averages](https://intranet.hbtn.io/rltoken/5N75PDrSPlBuQEXyV5lTuw)
* [Understanding Exponentially Weighted Averages](https://intranet.hbtn.io/rltoken/V1fGt--3DYdXIlFaZKxQ1Q)
* [Bias Correction of Exponentially Weighted Averages](https://intranet.hbtn.io/rltoken/F4Of4Km8QRl2mH6iCdGReg)
* [Gradient Descent With Momentum](https://intranet.hbtn.io/rltoken/DwaovproRxolK5BN2LTbQQ)
* [RMSProp](https://intranet.hbtn.io/rltoken/knRX814HFUQcumxnOOJSyw)
* [Adam Optimization Algorithm](https://intranet.hbtn.io/rltoken/c9O01hgfn3335zzQPGtlkQ)
* [Learning Rate Decay](https://intranet.hbtn.io/rltoken/PXmH63ae5SdNBSvwZOcN5w)
* [Normalizing Activations in a Network](https://intranet.hbtn.io/rltoken/bbhczA5i6hu1KC1SVFZf1g)
* [Fitting Batch Norm Into Neural Networks](https://intranet.hbtn.io/rltoken/tjvojWwSp0hhFontTO7ygw)
* [Why Does Batch Norm Work?](https://intranet.hbtn.io/rltoken/14HrGT4EmpD5lhQThvFOhg)
* [Batch Norm At Test Time](https://intranet.hbtn.io/rltoken/RQob4hYaNfjmDDeW7j49bA)
* [The Problem of Local Optima](https://intranet.hbtn.io/rltoken/mHsAE3RUtXZ0UQTOgtab9A)

---
## Learning Objectives:bulb:
What you should learn from this project:

* What is a hyperparameter?
* How and why do you normalize your input data?
* What is a saddle point?
* What is stochastic gradient descent?
* What is mini-batch gradient descent?
* What is a moving average? How do you implement it?
* What is gradient descent with momentum? How do you implement it?
* What is RMSProp? How do you implement it?
* What is Adam optimization? How do you implement it?
* What is learning rate decay? How do you implement it?
* What is batch normalization? How do you implement it?

---
---

## Author
* **Gabriel Cifuentes** - [gcifuentess](https://github.com/gcifuentess)